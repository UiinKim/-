{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_ErFzCXQpeMJ",
        "abHK6C_cqG2N",
        "HWHg6d2IuZ6w",
        "wtKWSaUaUj99",
        "yywxAHEZiJmF",
        "ZSRR7g8xjIGm",
        "ybN2tJP-IKcx",
        "epmYp86RIOwF",
        "uG-rZF72IVf0",
        "iU6kTvQxIZ3h"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/Crawling/blob/main/2%EC%9D%BC%EC%B0%A8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%88%98%EC%A7%91_%EB%B0%A9%EB%B2%95%EB%A1%A0_%ED%81%AC%EB%A1%A4%EB%A7%81%EC%9D%84_%ED%86%B5%ED%95%9C_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%88%98%EC%A7%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2일차_데이터_수집_방법론_크롤링을_통한_데이터_수집\n",
        "-----------\n",
        "\n",
        "## 실습 가이드\n",
        "1. 코드 실행은 Shift + Enter\n",
        "2. 코드는 위에서부터 아래로 순서대로 실행한다.\n",
        "\n",
        "## 데이터 설명\n"
      ],
      "metadata": {
        "id": "BOFj0UO-bSq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 웹 크롤링 실습"
      ],
      "metadata": {
        "id": "8x6LJkFYpPDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- HTML 구조 실습"
      ],
      "metadata": {
        "id": "ZgZ4b7bnnZOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vPar36-YGxt4"
      },
      "outputs": [],
      "source": [
        "html_doc ='''<html><head><title>웹크롤링 연습</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
        "<p class=\"link\">인터넷 링크를 작성하는 방법은\n",
        "<a class=\"naver\" href='https://sports.news.naver.com/wfootball/index' id=\"link1\">네이버 스포츠 뉴스</a>와\n",
        "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
        "HTML 링크로 작성할 수 있습니다.\n",
        "</p>\n",
        "<p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
        "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
        "</p>\n",
        "</body></html>\n",
        "'''\n",
        "\n",
        "with open('웹크롤링 연습.html','w') as f:\n",
        "  f.write(html_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 코드 작성 이후 실행하고 코랩 탐색기에서 '웹크롤링 연습.html'을 다운받아서 실행해보기\n",
        "- 각 링크를 보며 어떤 구조로 연결이 되었는지 생각해보기\n",
        "- HTML 태그에 따른 구조를 생각하며 보기"
      ],
      "metadata": {
        "id": "vizasE_hb8qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. BeautifulSoup"
      ],
      "metadata": {
        "id": "ZUsO81ATm_tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. bs4 설치"
      ],
      "metadata": {
        "id": "uTQSzabkm_tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bs4를 import로 사용하기 위해 설치\n",
        "\n",
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f0c006-fd6f-4632-d991-d50485559e99",
        "id": "WmLS91ZTm_tm"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HTML 구조 실습에서 작성해서 저장한 html 문서를 다시 불러들여 문서를 파싱하여 사람이 이해하기 쉬운 언어로 변환하고 필요한 정보를 추출하는 과정"
      ],
      "metadata": {
        "id": "wGCn_4a5m_tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 문서 파싱(parsing)\n",
        "\n",
        "- 앞서 저장한 html 문서를 다시 읽어들여 html 태그의 형태를 그대로 유지하면서 파싱하는 방법인 BeautifulSoup 클래스를 사용하여 읽는다\n",
        "- 이를 사람이 식별하기 좋게 태그 단위로 출력하는 prettify 메소드를 활용한다"
      ],
      "metadata": {
        "id": "zBsvGybem_tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bs4 모듈에서 BeautifulSoup 클래스를 import하고 bs라는 약칭으로 사용\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# 이전에 생성한 HTML 파일 ('웹크롤링 연습.html')을 읽기 모드('r')로 열기\n",
        "with open('웹크롤링 연습.html', 'r') as f:\n",
        "    # HTML 파일의 내용을 읽어와서 변수 html_doc에 저장\n",
        "    html_doc = f.read()\n",
        "\n",
        "# BeautifulSoup 객체 생성 - html_doc을 'html.parser'를 사용하여 파싱\n",
        "soup = bs(html_doc, 'html.parser')\n",
        "\n",
        "# BeautifulSoup 객체 출력 (HTML 구조를 그대로 출력)\n",
        "print(soup)\n",
        "\n",
        "# prettify() 메서드를 사용해 HTML 문서를 들여쓰기 및 정리하여 출력\n",
        "print(soup.prettify())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f59001-0517-4c8f-fbf6-1b10aa0f24de",
        "id": "9nYb_-BQm_tn"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html><head><title>웹크롤링 연습</title></head>\n",
            "<body>\n",
            "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
            "<p class=\"link\">인터넷 링크를 작성하는 방법은\n",
            "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>와\n",
            "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "</p>\n",
            "<p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
            "</p>\n",
            "</body></html>\n",
            "\n",
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   웹크롤링 연습\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <p class=\"title\">\n",
            "   <b>\n",
            "    <h1>\n",
            "     HTML 연습\n",
            "    </h1>\n",
            "   </b>\n",
            "  </p>\n",
            "  <p class=\"link\">\n",
            "   인터넷 링크를 작성하는 방법은\n",
            "   <a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">\n",
            "    네이버 스포츠 뉴스\n",
            "   </a>\n",
            "   와\n",
            "   <a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">\n",
            "    다음 스포츠 뉴스\n",
            "   </a>\n",
            "   처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "  </p>\n",
            "  <p class=\"story\">\n",
            "   그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "   <h2>\n",
            "    크게\n",
            "   </h2>\n",
            "   작성할 수도 있고,\n",
            "   <h4>\n",
            "    작게\n",
            "   </h4>\n",
            "   작성할 수도 있습니다.\n",
            "  </p>\n",
            " </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- prettify()를 사용해 출력시키면 태그의 시작과 종료 위치, 상위 및 하위 태그의 종속 관계를 이해하기 쉬워진다.\n",
        "-  여기서 저장한 soup 객체는 html 태그가 그대로 살아있어서 이 태그들의 특성을 이용해 필요한 정보를 추출할 수 있다."
      ],
      "metadata": {
        "id": "YbdnORD4m_tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# soup 객체에서 모든 <p> 태그를 찾은 후 반복문을 통해 텍스트만 출력\n",
        "for p in soup.find_all('p'):\n",
        "    # 찾은 <p> 태그의 텍스트 내용을 출력\n",
        "    print(p.text)\n",
        "\n",
        "# soup 객체에서 CSS 선택자 'p'를 사용하여 <p> 태그를 찾은 후 텍스트 출력\n",
        "for p in soup.select('p'):\n",
        "    # 찾은 <p> 태그의 텍스트 내용을 출력\n",
        "    print(p.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53fb927-a098-44a0-f24f-a9ae1e9691df",
        "id": "ZrGbt0nGm_tn"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML 연습\n",
            "인터넷 링크를 작성하는 방법은\n",
            "네이버 스포츠 뉴스와\n",
            "다음 스포츠 뉴스처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "\n",
            "그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "크게작성할 수도 있고, 작게 작성할 수도 있습니다.\n",
            "\n",
            "HTML 연습\n",
            "인터넷 링크를 작성하는 방법은\n",
            "네이버 스포츠 뉴스와\n",
            "다음 스포츠 뉴스처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "\n",
            "그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "크게작성할 수도 있고, 작게 작성할 수도 있습니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과를 보면 soup 객체의 메소드인 title과 태그명들을 활용하여 필요한 부분만 정보를 추출할 수 있다.\n",
        "- find_all, select 함수를 이용하는 방법이 필요한 부분의 정보를 추출하는데 더 효율적이다"
      ],
      "metadata": {
        "id": "hlWBWb-Em_tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# soup 객체에서 모든 <p> 태그를 찾아 텍스트를 출력\n",
        "for p in soup.find_all('p'):\n",
        "    # 각 <p> 태그 내부의 텍스트를 출력\n",
        "    print(p.text)\n",
        "\n",
        "# soup 객체에서 CSS 선택자 'p'를 사용하여 <p> 태그를 찾아 텍스트를 출력\n",
        "for p in soup.select('p'):\n",
        "    # 각 <p> 태그 내부의 텍스트를 출력\n",
        "    print(p.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa48d89a-da09-4ecc-ecc5-913cd8be9ef4",
        "id": "6QkiRqmWm_to"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML 연습\n",
            "인터넷 링크를 작성하는 방법은\n",
            "네이버 스포츠 뉴스와\n",
            "다음 스포츠 뉴스처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "\n",
            "그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "크게작성할 수도 있고, 작게 작성할 수도 있습니다.\n",
            "\n",
            "HTML 연습\n",
            "인터넷 링크를 작성하는 방법은\n",
            "네이버 스포츠 뉴스와\n",
            "다음 스포츠 뉴스처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "\n",
            "그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "크게작성할 수도 있고, 작게 작성할 수도 있습니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. find 함수 활용"
      ],
      "metadata": {
        "id": "EQLbTaLAm_to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **find** 함수는 태그, 속성과 속성값을 활용해서 HTML 문서의 텍스트를 추출할 수 있다\n",
        "- **find_all** 함수는 여러 개의 내용을 리스트 형태로 가져올 수 있으며, 특정 하나의 태그만을 추출하려면 find 함수를 사용한다\n",
        "- 태그 내의 **class** 혹은 **id** 등에 해당하는 속성과 속성값을 활용하여 해당 태그의 내용을 추출할 수 있다."
      ],
      "metadata": {
        "id": "Ombd1ru2m_to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# soup 객체에서 모든 <p> 태그를 찾아 리스트 형태로 출력\n",
        "print(soup.find_all('p'))\n",
        "\n",
        "# soup 객체에서 첫 번째 <p> 태그를 찾아 출력\n",
        "print(soup.find('p'))\n",
        "\n",
        "# soup 객체에서 클래스 이름이 'title'인 요소를 찾아 출력\n",
        "print(soup.find(class_='title'))\n",
        "\n",
        "# soup 객체에서 속성이 'class'이고 값이 'python'인 요소를 찾아 출력\n",
        "print(soup.find(attrs={'class': 'python'}))\n",
        "\n",
        "# soup 객체에서 태그가 'a'이고, 속성 'id'가 'link1'인 요소를 찾아 출력\n",
        "print(soup.find('a', {'id': 'link1'}))\n",
        "\n",
        "# soup 객체에서 속성이 'id'이고 값이 'link2'인 요소를 찾아 출력\n",
        "print(soup.find(attrs={'id': 'link2'}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2725ddc-ec04-43f2-87f0-d5c4d94495bf",
        "id": "r3ucmSuPm_to"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<p class=\"title\"><b><h1>HTML 연습</h1></b></p>, <p class=\"link\">인터넷 링크를 작성하는 방법은\n",
            "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>와\n",
            "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "</p>, <p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
            "</p>]\n",
            "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
            "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
            "None\n",
            "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>\n",
            "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 기본적으로 태그 명을 사용하고, 여기에 클래스나 아이디 등의 속성과 그에 해당하는 속성값을 넣어 해당 태그의 텍스트를 추출할 수 있다.\n",
        "- 실제 값을 추출할 경우는 find_all은 여러개의 시리즈를 가지고 있으므로 for 문을 사용한다.\n",
        "- find의 경우는 단일값을 가지고 있기 때문에 속성값을 추출할 때는 get 함수를 이용하고, 태그에 둘러싸여 있는 텍스트를 추출하고자 할 때는 text 메소드를 활용하면 된다."
      ],
      "metadata": {
        "id": "HXvd16Efm_tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 <p> 태그 내부에서 <h1> 태그를 찾아 텍스트 출력\n",
        "print(soup.find('p').find('h1').text)\n",
        "\n",
        "# 첫 번째 <p> 태그의 'class' 속성 값을 가져와 출력\n",
        "print(soup.find_all('p')[0].get('class'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02ba622-dc7b-47a8-df9e-2dc52de6be6d",
        "id": "Lj4Otu8Jm_tp"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML 연습\n",
            "['title']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. CSS 선택자(Selector) 이용 : select 함수"
      ],
      "metadata": {
        "id": "lQ_URlydm_tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CSS(Cascading Style Sheets)에서 선택자는 디자인 스타일을 적용하고자 하는 HTML의 요소를 선택할 때 사용된다\n",
        "- 다양한 종류가 있으며 여러 개의 선택자를 통해서 하나의 태그를 공통으로 제어할 수 있다\n"
      ],
      "metadata": {
        "id": "yGTuVtWim_tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 태그 선택자, 클래스 선택자, 아이디 선택자가 있다\n",
        "- 우선 적용 순위로는 조금 더 구체적인 선택자가 우선순위로 적용된다\n",
        "- find_all 함수와 같은 기능으로는 select 함수 (여러 개의 시리즈)\n",
        "- find 함수와 같은 기능으로는 select_one 함수 (하나의 태그)"
      ],
      "metadata": {
        "id": "_2DP-qVzm_tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSS 선택자 'p'를 사용하여 모든 <p> 태그를 리스트 형태로 출력\n",
        "print(soup.select('p'))\n",
        "\n",
        "# CSS 선택자 'p'를 사용하여 첫 번째 <p> 태그를 찾아 출력\n",
        "print(soup.select_one('p'))\n",
        "\n",
        "# 클래스 이름이 'title'인 요소를 찾아 출력 (CSS 선택자 사용)\n",
        "print(soup.select_one('.title'))\n",
        "\n",
        "# 클래스 이름이 'python'인 요소를 찾아 출력\n",
        "print(soup.select_one('python'))\n",
        "\n",
        "# 태그 'a'와 id='link1' 속성을 가진 요소를 찾아 출력 (CSS 선택자 사용)\n",
        "print(soup.select_one('a#link1'))\n",
        "\n",
        "# id='link2' 속성을 가진 요소를 찾아 출력\n",
        "print(soup.select_one('#link2'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb0ccaf-bebc-45a9-f04a-a998d298658b",
        "id": "dbZO_RVum_tq"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<p class=\"title\"><b><h1>HTML 연습</h1></b></p>, <p class=\"link\">인터넷 링크를 작성하는 방법은\n",
            "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>와\n",
            "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "</p>, <p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
            "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
            "</p>]\n",
            "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
            "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
            "None\n",
            "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>\n",
            "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- find 함수와 다른 부분은 문법상 **'class', 'id'** 태그를 **'.'**과 **'#'**으로 대체하여 사용하며, 기능은 동일하다"
      ],
      "metadata": {
        "id": "QhRlPz3mm_tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 <p> 태그 내부에 있는 id='link1' 요소의 텍스트 출력\n",
        "print(soup.select_one('p > #link1').text)\n",
        "\n",
        "# 두 번째 <a> 태그의 'id' 속성 값을 가져와 출력\n",
        "print(soup.select('a')[1].get('id'))\n",
        "\n",
        "# 두 번째 <p> 태그의 텍스트 내용을 출력\n",
        "print(soup.select('p')[1].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579d5f71-51a9-4d70-b36d-c258e34c90c6",
        "id": "6tBkOr7-m_tq"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "네이버 스포츠 뉴스\n",
            "link2\n",
            "인터넷 링크를 작성하는 방법은\n",
            "네이버 스포츠 뉴스와\n",
            "다음 스포츠 뉴스처럼\n",
            "HTML 링크로 작성할 수 있습니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Requests, Urllib"
      ],
      "metadata": {
        "id": "bGnpm273HBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. robots.txt로 확인"
      ],
      "metadata": {
        "id": "ybN2tJP-IKcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 네이버 뉴스의 크롤링 가능 여부 : https://news.naver.com/robots.txt\n"
      ],
      "metadata": {
        "id": "1wgp-zYe6zKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# 로봇 배제 기준 확인 (robots.txt 파일 가져오기)\n",
        "resp = requests.get(\"https://news.naver.com/robots.txt\")\n",
        "print(resp.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYoJzUNeIJzv",
        "outputId": "ab97dd5d-2d1e-40d4-d17a-e779feb2fb67"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-agent: Yeti\n",
            "Allow: /main/imagemontage\n",
            "Disallow: /\n",
            "\n",
            "User-Agent: facebookexternalhit\n",
            "Disallow: /*/template\n",
            "\n",
            "User-Agent: Twitterbot\n",
            "Disallow: /*/template\n",
            "\n",
            "User-agent: *\n",
            "Disallow: /\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3가지의 User-Agent 제외하고 모든 부분에 대해 허용"
      ],
      "metadata": {
        "id": "4jqcgP5HdUrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. Requests\n",
        "\n",
        "- 데이터 전송시 딕셔너리 형태로 주고 받기에 별도로 디코딩 과정이 필요 없음\n",
        "- get 메소드로 해당 웹 페이지 정보 읽기 가능\n",
        "- 수신한 HTML 문서에 대한 encoding이나 headers 정보를 간단하게 파악 가능\n",
        "- 모든 텍스트 정보를 확인하기 위해 간단하게 text 메서드 활용"
      ],
      "metadata": {
        "id": "epmYp86RIOwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# 웹 페이지에 GET 요청을 보냄\n",
        "r = requests.get('https://news.naver.com/')\n",
        "\n",
        "# 인코딩 방식 출력\n",
        "print(\"Encoding:\", r.encoding)\n",
        "\n",
        "# 헤더에서 'Expires' 정보 출력\n",
        "print(\"Expires:\", r.headers.get('Expires'))\n",
        "\n",
        "# 헤더에서 'Content-Type' 정보 출력\n",
        "print(\"Content-Type:\", r.headers.get('Content-Type'))\n",
        "\n",
        "# 전체 헤더 정보 출력\n",
        "print(\"\\nHeaders:\")\n",
        "print(r.headers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QF2ISB8H8Td",
        "outputId": "a39c0aab-1dcb-4cbc-a5a9-34d7e54a71db"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: utf-8\n",
            "Expires: None\n",
            "Content-Type: text/html;charset=utf-8\n",
            "\n",
            "Headers:\n",
            "{'Cache-Control': 'no-cache, no-store', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'x-frame-options': 'SAMEORIGIN', 'Content-Language': 'ko-KR', 'Content-Encoding': 'gzip', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Headers': 'accept, content-type', 'Access-Control-Allow-Methods': 'GET, POST', 'Content-Type': 'text/html;charset=utf-8', 'referrer-policy': 'unsafe-url', 'Server': 'nfront', 'Date': 'Sun, 17 Nov 2024 11:58:40 GMT', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive, Transfer-Encoding', 'Vary': 'Accept-Encoding'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 본인의 개인 블로그에 접근하는 방법\n",
        "- 아이디와 패스워드 입력"
      ],
      "metadata": {
        "id": "Ar0SnzsUfEZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 웹 페이지에 GET 요청을 보냄\n",
        "r = requests.get('https://blog.naver.com/right98', auth=('아이디', '패스워드'))\n",
        "\n",
        "# 인코딩 방식 출력\n",
        "print(\"Encoding:\", r.encoding)\n",
        "\n",
        "# 헤더에서 'Expires' 정보 출력\n",
        "print(\"Expires:\", r.headers.get('Expires'))\n",
        "\n",
        "# 헤더에서 'Content-Type' 정보 출력\n",
        "print(\"Content-Type:\", r.headers.get('Content-Type'))\n",
        "\n",
        "# 전체 헤더 정보 출력\n",
        "print(\"\\nHeaders:\")\n",
        "print(r.headers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ryd_PbE0sF3H",
        "outputId": "c91a835c-ec4c-4b99-cd9c-98d30b109bff"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: UTF-8\n",
            "Expires: Thu, 01 Jan 1970 00:00:00 GMT\n",
            "Content-Type: text/html;charset=UTF-8\n",
            "\n",
            "Headers:\n",
            "{'Content-Type': 'text/html;charset=UTF-8', 'Cache-Control': 'no-cache', 'Expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'Content-Encoding': 'gzip', 'referrer-policy': 'unsafe-url', 'Server': 'nfront', 'Content-Length': '1270', 'Date': 'Sun, 17 Nov 2024 11:55:49 GMT', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'Set-Cookie': 'JSESSIONID=059DC3925546B04C04D32EE5EA57799C.jvm1; Path=/; Secure; HttpOnly'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- requests + bs4의 select 활용"
      ],
      "metadata": {
        "id": "lkNOGB6J-tzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# 웹 페이지 요청 (requests는 정적 페이지만 가능)\n",
        "response = requests.get('https://ai-bigdataanalysis.blogspot.com/2024/03/html.html')\n",
        "\n",
        "# 요청된 페이지의 HTML 내용을 텍스트 형태로 가져옴\n",
        "html_doc = response.text\n",
        "\n",
        "# BeautifulSoup 객체 생성 및 HTML 파싱\n",
        "soup = bs(html_doc, 'html.parser')\n",
        "\n",
        "# 특정 div 태그를 선택하여 해당 콘텐츠 가져오기 (클래스가 'post-body'인 div 요소 선택)\n",
        "content = soup.select_one('div.post-body')\n",
        "\n",
        "# 추출한 콘텐츠에서 모든 <span> 태그의 텍스트를 합쳐서 저장\n",
        "text = \"\"\n",
        "if content:  # 'content'가 None이 아닐 경우에만 실행\n",
        "    for para in content.select(\"span\"):\n",
        "        text += para.text.strip()\n",
        "\n",
        "# 최종적으로 텍스트 출력\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m25j_MMy-ypK",
        "outputId": "25d2b805-3e24-4259-b61d-3817e7007f8e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "참조 :https://wikidocs.net/1278461. HTML 구조의 이해HTML은 웹 문서를 만들기 위하여 사용하는 기본적인 웹 언어의 한 종류이다. 하이퍼텍스트를 작성하기 위해 개발되었으며 인터넷 서비스의 하나인 월드와이드웹(WWW)을 통해서 볼 수 있는 문서를 만들 때 사용하는 웹 언어의 한 종류이다. 특히 인터넷에서 웹을 통해 접근되는 대부분의 웹 페이지들은 HTML로 작성되는데 문서의 글자 크기, 글자색, 글자모양, 그래픽, 문서이동(하이퍼링크) 등을 정의하는 명령어들이 있다. HTML에서 사용하는 명령어는 태그(tag)라고 하는데 꺽쇠괄호'<>'를 사용하여 나타낸다. HTML 문서는 크게 Head 부분과 Body로 나누어지는데 Head 부분에는 문서의 제목을 포함한 기본적인 정보들을 포함하고 있으며 Body 부분에는 실제 웹 페이지에 나타나는 본문 내용으로 구성된다. 그래서 이러한 구조를 간단하게 요약해서 표기하면 아래와 같은 그림으로 표현할 수 있다.좀더 구체적으로 위에 그림 중에 본문 문단을 의미하는 p 태그의 코드 기준으로 설명하면 HTML 요소는 시작 태그와 /로 시작하는 종료 태그 사이에 내용들이 들어 있으며, 시작 태그 안에는 속성명(Attributes) 'class'와 속성값(Values) 'para'를 포함하고 있고, '빅데이터 분석과 활용'이라는 내용을 포함하고 있는 태그이다.보다 다양한 HTML 태그에 대한 이해를 위해서 자주 사용하는 태그들을 표의 형태로 정리하면 아래와 같다.이러한 구조를 사람이 보기 편한 Text 문서로 파싱하기 위해서는 BeautifulSoup라는 라이브러리를 이용해야 한다.이라는 파이썬 라이브러리는 이와 같이 HTML 문서를 파싱하여 텍스트 데이터를 추출할 수 있는 유용한 도구이다.2.HTML 문서의 사례와 텍스트 추출이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.와코랩을 이용해서 위에 그림으로 설명한 P 태그의 내용을 추출하는 방법을 설명하면 아래와 같은 코드를 실행시키면 추출이 가능하다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.!pip install bs4그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.fromimportas'''<p class='para'>빅데이터 분석과 활용</p>''''html.parser'print\"p\"print\"p\"\"class\"3. 웹크롤링 방법 사례3. 웹크롤링 방법 사례웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.가. 기본정보 추출가. 기본정보 추출기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.import requestsimport requestsfrom bs4 import BeautifulSoup as bsfrom bs4 import BeautifulSoup as bsurl = \"https://enjoystat.blogspot.com/2024/03/html.html\"url = \"https://enjoystat.blogspot.com/2024/03/html.html\"resp = requests.get(url)resp = requests.get(url)print(resp)print(resp)html_src = resp.texthtml_src = resp.textsoup = bs(html_src, 'html.parser')soup = bs(html_src, 'html.parser')나. 태그사이에 있는 정보 추출나. 태그사이에 있는 정보 추출태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()다. 속성값 추출다. 속성값 추출태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))라. 반복되는 내용 추출라. 반복되는 내용 추출웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.text =\"\"text =\"\"body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})for para in body.select(\"p\"):for para in body.select(\"p\"):text=text+para.text.strip()text=text+para.text.strip()texttext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. urllib"
      ],
      "metadata": {
        "id": "uG-rZF72IVf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- requests 라이브러리와 다르게 데이터 전송시 바이너리 데이터로 변환하여 주고 받기에 HTML 문서를 수신했을 때 디코딩 과정이 필요하다\n",
        "- 한글은 UTF-8, EUC-KR, cp949 형태가 존재\n",
        "- HTML 문서의 Head 부분에서 관련된 정보를 찾아 적절한 포맷에 맞게 디코딩을 해야한다\n"
      ],
      "metadata": {
        "id": "Yzc3nRNGgKP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urllib.request 모듈에서 urlopen 함수를 import\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# 웹 페이지 요청 및 응답 객체 가져오기\n",
        "# 'https://ai-bigdataanalysis.blogspot.com' 페이지에 접근하여 응답 객체를 html 변수에 저장\n",
        "html = urlopen('https://ai-bigdataanalysis.blogspot.com')\n",
        "\n",
        "# 응답된 페이지의 인코딩 방식 확인\n",
        "# 서버에서 응답한 HTTP 헤더에서 'Content-Type'의 charset 값을 가져와 인코딩 방식을 출력\n",
        "print(\"Encoding:\", html.headers.get_content_charset())\n",
        "\n",
        "# HTML 내용을 읽어와서 디코딩 (기본적으로 UTF-8로 인코딩된 경우)\n",
        "# 서버로부터 받은 데이터를 바이너리 형식으로 읽어온 후, 문자열로 디코딩하여 저장\n",
        "html_doc = html.read().decode()\n",
        "\n",
        "# 디코딩된 HTML 문서의 처음 200자를 출력\n",
        "# HTML 소스의 일부를 출력하여 페이지 구조를 확인\n",
        "print(html_doc[:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g4n3HmQH8fz",
        "outputId": "ae8d441d-2d5c-4f01-d5f2-8395a333007e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: utf-8\n",
            "<!DOCTYPE html>\n",
            "<html class='v2' dir='ltr' lang='ko'>\n",
            "<head>\n",
            "<link href='https://www.blogger.com/static/v1/widgets/3566091532-css_bundle_v2.css' rel='stylesheet' type='text/css'/>\n",
            "<meta content='width\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- UTF-8이 아닌 EUC-KR이나 cp949일경우 decode('cp949')로 변경\n",
        "- urllib과 bs4의 find 문 활용한 웹 크롤링 방식"
      ],
      "metadata": {
        "id": "bvcxUeQAg50b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urllib 라이브러리에서 urlopen 함수를 import\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# BeautifulSoup 모듈에서 BeautifulSoup 클래스를 bs라는 이름으로 import\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# 웹 페이지 요청 및 응답 객체 가져오기\n",
        "# 'https://ai-bigdataanalysis.blogspot.com/2024/03/html.html' 페이지에 접근하여 응답 객체를 response 변수에 저장\n",
        "response = urlopen('https://ai-bigdataanalysis.blogspot.com/2024/03/html.html')\n",
        "\n",
        "# 응답된 HTML 문서를 읽어와서 바이너리 데이터를 문자열로 디코딩\n",
        "html_doc = response.read().decode()\n",
        "\n",
        "# BeautifulSoup 객체 생성 및 HTML 파싱\n",
        "# HTML 소스를 BeautifulSoup을 사용하여 파싱하고 soup 변수에 저장\n",
        "soup = bs(html_doc, 'html.parser')\n",
        "\n",
        "# 특정 div 태그를 선택하여 해당 콘텐츠 가져오기\n",
        "# 클래스가 'post-body entry-content'인 div 태그를 찾음\n",
        "content = soup.find(\"div\", {\"class\": \"post-body entry-content\"})\n",
        "\n",
        "# 텍스트를 저장할 변수 초기화\n",
        "text = \"\"\n",
        "\n",
        "# 찾은 div 태그 내부에서 모든 <span> 태그를 순회하면서 텍스트를 추출\n",
        "if content:  # content가 None이 아닐 경우에만 실행\n",
        "    for para in content.find_all(\"span\"):\n",
        "        # 각 <span> 태그의 텍스트를 가져와서 양쪽 공백 제거 후 text 변수에 추가\n",
        "        text += para.text.strip()\n",
        "\n",
        "# 최종적으로 수집된 텍스트 출력\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "YHp5i4UO-Wrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093eb099-ebbb-4a4c-bc1d-15420e3d9aae"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "참조 :https://wikidocs.net/1278461. HTML 구조의 이해HTML은 웹 문서를 만들기 위하여 사용하는 기본적인 웹 언어의 한 종류이다. 하이퍼텍스트를 작성하기 위해 개발되었으며 인터넷 서비스의 하나인 월드와이드웹(WWW)을 통해서 볼 수 있는 문서를 만들 때 사용하는 웹 언어의 한 종류이다. 특히 인터넷에서 웹을 통해 접근되는 대부분의 웹 페이지들은 HTML로 작성되는데 문서의 글자 크기, 글자색, 글자모양, 그래픽, 문서이동(하이퍼링크) 등을 정의하는 명령어들이 있다. HTML에서 사용하는 명령어는 태그(tag)라고 하는데 꺽쇠괄호'<>'를 사용하여 나타낸다. HTML 문서는 크게 Head 부분과 Body로 나누어지는데 Head 부분에는 문서의 제목을 포함한 기본적인 정보들을 포함하고 있으며 Body 부분에는 실제 웹 페이지에 나타나는 본문 내용으로 구성된다. 그래서 이러한 구조를 간단하게 요약해서 표기하면 아래와 같은 그림으로 표현할 수 있다.좀더 구체적으로 위에 그림 중에 본문 문단을 의미하는 p 태그의 코드 기준으로 설명하면 HTML 요소는 시작 태그와 /로 시작하는 종료 태그 사이에 내용들이 들어 있으며, 시작 태그 안에는 속성명(Attributes) 'class'와 속성값(Values) 'para'를 포함하고 있고, '빅데이터 분석과 활용'이라는 내용을 포함하고 있는 태그이다.보다 다양한 HTML 태그에 대한 이해를 위해서 자주 사용하는 태그들을 표의 형태로 정리하면 아래와 같다.이러한 구조를 사람이 보기 편한 Text 문서로 파싱하기 위해서는 BeautifulSoup라는 라이브러리를 이용해야 한다.이라는 파이썬 라이브러리는 이와 같이 HTML 문서를 파싱하여 텍스트 데이터를 추출할 수 있는 유용한 도구이다.2.HTML 문서의 사례와 텍스트 추출이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.와코랩을 이용해서 위에 그림으로 설명한 P 태그의 내용을 추출하는 방법을 설명하면 아래와 같은 코드를 실행시키면 추출이 가능하다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.!pip install bs4그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.fromimportas'''<p class='para'>빅데이터 분석과 활용</p>''''html.parser'print\"p\"print\"p\"\"class\"3. 웹크롤링 방법 사례3. 웹크롤링 방법 사례웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.가. 기본정보 추출가. 기본정보 추출기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.import requestsimport requestsfrom bs4 import BeautifulSoup as bsfrom bs4 import BeautifulSoup as bsurl = \"https://enjoystat.blogspot.com/2024/03/html.html\"url = \"https://enjoystat.blogspot.com/2024/03/html.html\"resp = requests.get(url)resp = requests.get(url)print(resp)print(resp)html_src = resp.texthtml_src = resp.textsoup = bs(html_src, 'html.parser')soup = bs(html_src, 'html.parser')나. 태그사이에 있는 정보 추출나. 태그사이에 있는 정보 추출태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()다. 속성값 추출다. 속성값 추출태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))라. 반복되는 내용 추출라. 반복되는 내용 추출웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.text =\"\"text =\"\"body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})for para in body.select(\"p\"):for para in body.select(\"p\"):text=text+para.text.strip()text=text+para.text.strip()texttext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Selenium"
      ],
      "metadata": {
        "id": "TKozpKCwD_Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. selenium 라이브러리 설치"
      ],
      "metadata": {
        "id": "OSOAyTz-D_Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 웹 브라우저에 필요한 드라이버 사전 설치가 필요하다\n",
        "- 반드시 웹 브라우저의 종류와 version 정보가 일치해야 한다\n"
      ],
      "metadata": {
        "id": "LfzCzcrjD_Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롬 드라이버(chromedriver)를 설치하기 위한 명령어\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "# Selenium을 설치하고, chromedriver_autoinstaller 라이브러리를 설치\n",
        "!pip install chromedriver_autoinstaller selenium\n",
        "\n",
        "# 시스템 모듈을 가져옴\n",
        "import sys\n",
        "import chromedriver_autoinstaller\n",
        "\n",
        "# 시스템 경로에 크롬 드라이버 경로를 추가 (드라이버를 찾기 위해)\n",
        "sys.path.insert(0, '/usr/bin/chromedriver')\n",
        "\n",
        "# chromedriver_autoinstaller를 사용하여 현재 설치된 Chrome 버전에 맞는 드라이버 자동 설치\n",
        "chromedriver_autoinstaller.install()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9caaefa9-24b6-4830-997c-ae4bd139a322",
        "id": "A6UEpEidD_Dl"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 29.0 MB of archives.\n",
            "After this operation, 120 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.65.3+22.04 [26.4 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 29.0 MB in 3s (9,847 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123837 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.65.3+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.65.3+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.65.3+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124066 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting chromedriver_autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver_autoinstaller) (24.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, chromedriver_autoinstaller, trio, trio-websocket, selenium\n",
            "Successfully installed chromedriver_autoinstaller-0.6.4 outcome-1.3.0.post0 selenium-4.26.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. Selenium을 활용한 웹 크롤링\n",
        "- 코랩 환경에서는 웹 브라우저에 대한 핸들링이 직접적으로 불가능하다\n",
        "- 아래와 같은 옵션들을 크롬 웹 드라이버에 적용시켜야 한다"
      ],
      "metadata": {
        "id": "DWgsCd8ND_Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "# 크롬 브라우저를 headless 모드로 실행하기 위한 옵션 설정\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # GUI(그래픽 사용자 인터페이스) 없이 실행 (백그라운드에서 실행)\n",
        "chrome_options.add_argument('--no-sandbox')  # 리눅스 환경에서 샌드박스 모드 비활성화 (권한 문제 해결)\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # /dev/shm 사용 비활성화 (메모리 문제 방지)\n",
        "\n",
        "# 설정된 옵션을 사용하여 Chrome WebDriver 객체 생성\n",
        "driver = webdriver.Chrome(options=chrome_options)\n"
      ],
      "metadata": {
        "id": "ry4BVngED_Dm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- daum 사이트에서 인공지능을 검색하여 웹 크롤링한 사례"
      ],
      "metadata": {
        "id": "b7TDYWSGD_Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 지정된 URL로 이동 (Wikidocs 페이지)\n",
        "driver.get(\"https://news.naver.com/\")\n",
        "\n",
        "# 페이지 로딩을 위해 3초간 대기 (강제 대기)\n",
        "time.sleep(3)\n",
        "\n",
        "# 암묵적 대기 설정 (최대 1초 동안 요소를 기다림)\n",
        "driver.implicitly_wait(1)\n",
        "\n",
        "# 검색 입력란을 찾아서 키워드 '웹크롤링' 입력 (XPath를 이용)\n",
        "driver.find_element(By.XPATH, '//*[@id=\"kw\"]').send_keys('축구')\n",
        "\n",
        "# 검색 버튼을 클릭 (XPath를 이용하여 버튼 찾기)\n",
        "# 기존 CSS Selector로 주석 처리된 부분을 대신하여, XPath로 요소를 찾아 클릭합니다.\n",
        "driver.find_element(By.XPATH, \"/html/body/div/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/button[1]\").click()\n",
        "\n",
        "# 검색 결과가 로드될 시간을 위해 3초 대기\n",
        "time.sleep(3)\n",
        "\n",
        "# 검색 결과의 제목을 담고 있는 요소들을 클래스 이름으로 찾음\n",
        "news_titles = driver.find_elements(By.CLASS_NAME, \"find-title\")\n",
        "\n",
        "# 검색 결과의 제목을 순회하면서 텍스트를 출력\n",
        "for i in news_titles:\n",
        "    title = i.text\n",
        "    print(title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "outputId": "bcccdaf8-486e-4e6b-efc9-8470bfa7bdda",
        "id": "wM17vPZWD_Dm"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NoSuchElementException",
          "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"kw\"]\"}\n  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n#0 0x57d519e284ca <unknown>\n#1 0x57d51993b620 <unknown>\n#2 0x57d51998a306 <unknown>\n#3 0x57d51998a5a1 <unknown>\n#4 0x57d5199cfa04 <unknown>\n#5 0x57d5199ae39d <unknown>\n#6 0x57d5199ccde0 <unknown>\n#7 0x57d5199ae113 <unknown>\n#8 0x57d51997cbe0 <unknown>\n#9 0x57d51997dbbe <unknown>\n#10 0x57d519df4e4b <unknown>\n#11 0x57d519df8de2 <unknown>\n#12 0x57d519de1d2c <unknown>\n#13 0x57d519df9957 <unknown>\n#14 0x57d519dc74bf <unknown>\n#15 0x57d519e17348 <unknown>\n#16 0x57d519e17510 <unknown>\n#17 0x57d519e27346 <unknown>\n#18 0x131103ffeac3 <unknown>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-ef6a093ba3a0>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 검색 입력란을 찾아서 키워드 '웹크롤링' 입력 (XPath를 이용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'//*[@id=\"kw\"]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'축구'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 검색 버튼을 클릭 (XPath를 이용하여 버튼 찾기)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFIND_ELEMENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"using\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWebElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"kw\"]\"}\n  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n#0 0x57d519e284ca <unknown>\n#1 0x57d51993b620 <unknown>\n#2 0x57d51998a306 <unknown>\n#3 0x57d51998a5a1 <unknown>\n#4 0x57d5199cfa04 <unknown>\n#5 0x57d5199ae39d <unknown>\n#6 0x57d5199ccde0 <unknown>\n#7 0x57d5199ae113 <unknown>\n#8 0x57d51997cbe0 <unknown>\n#9 0x57d51997dbbe <unknown>\n#10 0x57d519df4e4b <unknown>\n#11 0x57d519df8de2 <unknown>\n#12 0x57d519de1d2c <unknown>\n#13 0x57d519df9957 <unknown>\n#14 0x57d519dc74bf <unknown>\n#15 0x57d519e17348 <unknown>\n#16 0x57d519e17510 <unknown>\n#17 0x57d519e27346 <unknown>\n#18 0x131103ffeac3 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- requests + bs4의 select 활용"
      ],
      "metadata": {
        "id": "28AqQGCtD_Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# 웹 페이지 요청 (requests는 정적 페이지만 가능)\n",
        "response = requests.get('https://ai-bigdataanalysis.blogspot.com/2024/03/html.html')\n",
        "\n",
        "# 요청된 페이지의 HTML 내용을 텍스트 형태로 가져옴\n",
        "html_doc = response.text\n",
        "\n",
        "# BeautifulSoup 객체 생성 및 HTML 파싱\n",
        "soup = bs(html_doc, 'html.parser')\n",
        "\n",
        "# 특정 div 태그를 선택하여 해당 콘텐츠 가져오기 (클래스가 'post-body'인 div 요소 선택)\n",
        "content = soup.select_one('div.post-body')\n",
        "\n",
        "# 추출한 콘텐츠에서 모든 <span> 태그의 텍스트를 합쳐서 저장\n",
        "text = \"\"\n",
        "if content:  # 'content'가 None이 아닐 경우에만 실행\n",
        "    for para in content.select(\"span\"):\n",
        "        text += para.text.strip()\n",
        "\n",
        "# 최종적으로 텍스트 출력\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d2b805-3e24-4259-b61d-3817e7007f8e",
        "id": "UyOKioLdD_Dm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "참조 :https://wikidocs.net/1278461. HTML 구조의 이해HTML은 웹 문서를 만들기 위하여 사용하는 기본적인 웹 언어의 한 종류이다. 하이퍼텍스트를 작성하기 위해 개발되었으며 인터넷 서비스의 하나인 월드와이드웹(WWW)을 통해서 볼 수 있는 문서를 만들 때 사용하는 웹 언어의 한 종류이다. 특히 인터넷에서 웹을 통해 접근되는 대부분의 웹 페이지들은 HTML로 작성되는데 문서의 글자 크기, 글자색, 글자모양, 그래픽, 문서이동(하이퍼링크) 등을 정의하는 명령어들이 있다. HTML에서 사용하는 명령어는 태그(tag)라고 하는데 꺽쇠괄호'<>'를 사용하여 나타낸다. HTML 문서는 크게 Head 부분과 Body로 나누어지는데 Head 부분에는 문서의 제목을 포함한 기본적인 정보들을 포함하고 있으며 Body 부분에는 실제 웹 페이지에 나타나는 본문 내용으로 구성된다. 그래서 이러한 구조를 간단하게 요약해서 표기하면 아래와 같은 그림으로 표현할 수 있다.좀더 구체적으로 위에 그림 중에 본문 문단을 의미하는 p 태그의 코드 기준으로 설명하면 HTML 요소는 시작 태그와 /로 시작하는 종료 태그 사이에 내용들이 들어 있으며, 시작 태그 안에는 속성명(Attributes) 'class'와 속성값(Values) 'para'를 포함하고 있고, '빅데이터 분석과 활용'이라는 내용을 포함하고 있는 태그이다.보다 다양한 HTML 태그에 대한 이해를 위해서 자주 사용하는 태그들을 표의 형태로 정리하면 아래와 같다.이러한 구조를 사람이 보기 편한 Text 문서로 파싱하기 위해서는 BeautifulSoup라는 라이브러리를 이용해야 한다.이라는 파이썬 라이브러리는 이와 같이 HTML 문서를 파싱하여 텍스트 데이터를 추출할 수 있는 유용한 도구이다.2.HTML 문서의 사례와 텍스트 추출이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.와코랩을 이용해서 위에 그림으로 설명한 P 태그의 내용을 추출하는 방법을 설명하면 아래와 같은 코드를 실행시키면 추출이 가능하다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.!pip install bs4그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.fromimportas'''<p class='para'>빅데이터 분석과 활용</p>''''html.parser'print\"p\"print\"p\"\"class\"3. 웹크롤링 방법 사례3. 웹크롤링 방법 사례웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.가. 기본정보 추출가. 기본정보 추출기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.import requestsimport requestsfrom bs4 import BeautifulSoup as bsfrom bs4 import BeautifulSoup as bsurl = \"https://enjoystat.blogspot.com/2024/03/html.html\"url = \"https://enjoystat.blogspot.com/2024/03/html.html\"resp = requests.get(url)resp = requests.get(url)print(resp)print(resp)html_src = resp.texthtml_src = resp.textsoup = bs(html_src, 'html.parser')soup = bs(html_src, 'html.parser')나. 태그사이에 있는 정보 추출나. 태그사이에 있는 정보 추출태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()다. 속성값 추출다. 속성값 추출태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))라. 반복되는 내용 추출라. 반복되는 내용 추출웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.text =\"\"text =\"\"body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})for para in body.select(\"p\"):for para in body.select(\"p\"):text=text+para.text.strip()text=text+para.text.strip()texttext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. urllib"
      ],
      "metadata": {
        "id": "v3hIQXvvD_Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- requests 라이브러리와 다르게 데이터 전송시 바이너리 데이터로 변환하여 주고 받기에 HTML 문서를 수신했을 때 디코딩 과정이 필요하다\n",
        "- 한글은 UTF-8, EUC-KR, cp949 형태가 존재\n",
        "- HTML 문서의 Head 부분에서 관련된 정보를 찾아 적절한 포맷에 맞게 디코딩을 해야한다\n"
      ],
      "metadata": {
        "id": "t4H4hNuJD_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urllib.request 모듈에서 urlopen 함수를 import\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# 웹 페이지 요청 및 응답 객체 가져오기\n",
        "# 'https://ai-bigdataanalysis.blogspot.com' 페이지에 접근하여 응답 객체를 html 변수에 저장\n",
        "html = urlopen('https://ai-bigdataanalysis.blogspot.com')\n",
        "\n",
        "# 응답된 페이지의 인코딩 방식 확인\n",
        "# 서버에서 응답한 HTTP 헤더에서 'Content-Type'의 charset 값을 가져와 인코딩 방식을 출력\n",
        "print(\"Encoding:\", html.headers.get_content_charset())\n",
        "\n",
        "# HTML 내용을 읽어와서 디코딩 (기본적으로 UTF-8로 인코딩된 경우)\n",
        "# 서버로부터 받은 데이터를 바이너리 형식으로 읽어온 후, 문자열로 디코딩하여 저장\n",
        "html_doc = html.read().decode()\n",
        "\n",
        "# 디코딩된 HTML 문서의 처음 200자를 출력\n",
        "# HTML 소스의 일부를 출력하여 페이지 구조를 확인\n",
        "print(html_doc[:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8d441d-2d5c-4f01-d5f2-8395a333007e",
        "id": "GnfWFzw-D_Dn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: utf-8\n",
            "<!DOCTYPE html>\n",
            "<html class='v2' dir='ltr' lang='ko'>\n",
            "<head>\n",
            "<link href='https://www.blogger.com/static/v1/widgets/3566091532-css_bundle_v2.css' rel='stylesheet' type='text/css'/>\n",
            "<meta content='width\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- UTF-8이 아닌 EUC-KR이나 cp949일경우 decode('cp949')로 변경\n",
        "- urllib과 bs4의 find 문 활용한 웹 크롤링 방식"
      ],
      "metadata": {
        "id": "1N6sDqAoD_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urllib 라이브러리에서 urlopen 함수를 import\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# BeautifulSoup 모듈에서 BeautifulSoup 클래스를 bs라는 이름으로 import\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# 웹 페이지 요청 및 응답 객체 가져오기\n",
        "# 'https://ai-bigdataanalysis.blogspot.com/2024/03/html.html' 페이지에 접근하여 응답 객체를 response 변수에 저장\n",
        "response = urlopen('https://ai-bigdataanalysis.blogspot.com/2024/03/html.html')\n",
        "\n",
        "# 응답된 HTML 문서를 읽어와서 바이너리 데이터를 문자열로 디코딩\n",
        "html_doc = response.read().decode()\n",
        "\n",
        "# BeautifulSoup 객체 생성 및 HTML 파싱\n",
        "# HTML 소스를 BeautifulSoup을 사용하여 파싱하고 soup 변수에 저장\n",
        "soup = bs(html_doc, 'html.parser')\n",
        "\n",
        "# 특정 div 태그를 선택하여 해당 콘텐츠 가져오기\n",
        "# 클래스가 'post-body entry-content'인 div 태그를 찾음\n",
        "content = soup.find(\"div\", {\"class\": \"post-body entry-content\"})\n",
        "\n",
        "# 텍스트를 저장할 변수 초기화\n",
        "text = \"\"\n",
        "\n",
        "# 찾은 div 태그 내부에서 모든 <span> 태그를 순회하면서 텍스트를 추출\n",
        "if content:  # content가 None이 아닐 경우에만 실행\n",
        "    for para in content.find_all(\"span\"):\n",
        "        # 각 <span> 태그의 텍스트를 가져와서 양쪽 공백 제거 후 text 변수에 추가\n",
        "        text += para.text.strip()\n",
        "\n",
        "# 최종적으로 수집된 텍스트 출력\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093eb099-ebbb-4a4c-bc1d-15420e3d9aae",
        "id": "Z34pJxpnD_Do"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "참조 :https://wikidocs.net/1278461. HTML 구조의 이해HTML은 웹 문서를 만들기 위하여 사용하는 기본적인 웹 언어의 한 종류이다. 하이퍼텍스트를 작성하기 위해 개발되었으며 인터넷 서비스의 하나인 월드와이드웹(WWW)을 통해서 볼 수 있는 문서를 만들 때 사용하는 웹 언어의 한 종류이다. 특히 인터넷에서 웹을 통해 접근되는 대부분의 웹 페이지들은 HTML로 작성되는데 문서의 글자 크기, 글자색, 글자모양, 그래픽, 문서이동(하이퍼링크) 등을 정의하는 명령어들이 있다. HTML에서 사용하는 명령어는 태그(tag)라고 하는데 꺽쇠괄호'<>'를 사용하여 나타낸다. HTML 문서는 크게 Head 부분과 Body로 나누어지는데 Head 부분에는 문서의 제목을 포함한 기본적인 정보들을 포함하고 있으며 Body 부분에는 실제 웹 페이지에 나타나는 본문 내용으로 구성된다. 그래서 이러한 구조를 간단하게 요약해서 표기하면 아래와 같은 그림으로 표현할 수 있다.좀더 구체적으로 위에 그림 중에 본문 문단을 의미하는 p 태그의 코드 기준으로 설명하면 HTML 요소는 시작 태그와 /로 시작하는 종료 태그 사이에 내용들이 들어 있으며, 시작 태그 안에는 속성명(Attributes) 'class'와 속성값(Values) 'para'를 포함하고 있고, '빅데이터 분석과 활용'이라는 내용을 포함하고 있는 태그이다.보다 다양한 HTML 태그에 대한 이해를 위해서 자주 사용하는 태그들을 표의 형태로 정리하면 아래와 같다.이러한 구조를 사람이 보기 편한 Text 문서로 파싱하기 위해서는 BeautifulSoup라는 라이브러리를 이용해야 한다.이라는 파이썬 라이브러리는 이와 같이 HTML 문서를 파싱하여 텍스트 데이터를 추출할 수 있는 유용한 도구이다.2.HTML 문서의 사례와 텍스트 추출이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.이해를 돕기위해 아래와 같이 HTML 소스코드를 아래와 같이 작성하여 '웹크롤링 연습.html'로 파일을 저장하고 웹브라우저로 열어보면 HTML로 작성된 문서의 규칙을 이해할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.<html><head><title>웹크롤링 연습</title></head><body><p class=\"title\"><b><h1>HTML 연습</h1></b></p><p class=\"link\">인터넷 링크를 작성하는 방법은<a class=\"blog\" href='https://ai-bigdataanalysis.blogspot.com/' id=\"link1\">인공지능 기반 덱스트 데이터 분석 블로그</a>와<a class=\"wikidocs\" href=\"https://wikidocs.net/book/6280\" id=\"link2\">인공지능 기반 빅데이터 자연어 처리와 통계 분석</a>처럼HTML 링크로 작성할 수 있습니다.</p><p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.</p></body></html>위에 있는 소스코드로 작성된 HTML 파일을 웹브라우저로 열면 아래와 같은 형태의 웹페이지로 출력됨을 알 수 있다. 특히 여기서 인공지능 기반 덱스트 데이터 분석 블로그와 인공지능 기반 빅데이터 자연어 처리와 통계 분석의 경우는 a 태그에서 href 속성을 이용한 링크로 연결되어 다른 웹페이지로 이동할 수 있다.와코랩을 이용해서 위에 그림으로 설명한 P 태그의 내용을 추출하는 방법을 설명하면 아래와 같은 코드를 실행시키면 추출이 가능하다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.먼저 뷰티풀소프라는 라이브러리를 아래와 같이 설치한다.!pip install bs4그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.그리고 태그를 파싱해서 그안에 담겨있는 텍스트를 추출하거나 속성에 해당하는 속성값을 추출는 방법을 정리하면 아래와 같다.fromimportas'''<p class='para'>빅데이터 분석과 활용</p>''''html.parser'print\"p\"print\"p\"\"class\"3. 웹크롤링 방법 사례3. 웹크롤링 방법 사례웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.웹크롤링을 하기 위해 우선적으로 해야할 일은 우리가 정보를 추출하고자하는 웹페이지의 로봇배제기준을 확인하여 추출이 가능한 웹페이지인지 확인해야한다. 현재 이 블로그의 로봇배제기준을 확인하려면 웹페이지 주소창에 있는 주소에 추가하여 /robots.txt 를 추가하여 확인하는 것이다.가. 기본정보 추출가. 기본정보 추출기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.기본적인 웹페이지의 정보를 크롤링하기 위해서는 아래와 같이 requests나 urllib 라이브러리를 이용하여 HTML 정보를 불러오고 이를 BeautifulSoup으로 파싱하여 구체적인 텍스트 정보를 얻을 수있다.import requestsimport requestsfrom bs4 import BeautifulSoup as bsfrom bs4 import BeautifulSoup as bsurl = \"https://enjoystat.blogspot.com/2024/03/html.html\"url = \"https://enjoystat.blogspot.com/2024/03/html.html\"resp = requests.get(url)resp = requests.get(url)print(resp)print(resp)html_src = resp.texthtml_src = resp.textsoup = bs(html_src, 'html.parser')soup = bs(html_src, 'html.parser')나. 태그사이에 있는 정보 추출나. 태그사이에 있는 정보 추출태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.태그사이에 있는 정보를 추출하기 위해서는 f12 또는 Ctrl+Shift+I로 소스코드가 팝업되도록 펼치고, 좌상단 element select 또는 Ctrl+Shift+C로 마우스 선택기를 활성화하여 추출하고자하는 행에서 우측마우 클릭하고, copy - copy selector 선택 복사하여 아래와 같이 Select_one문에 붙여넣기하여 기본정보를 추출할 수 있다.soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()soup.select_one(\"#Blog1 > div.blog-posts.hfeed > div > div > div > div.post.hentry.uncustomized-post-template > h3\").text.strip()다. 속성값 추출다. 속성값 추출태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.태그 내에 작성되어 있는 속성값을 추출하기 위해서는 get(\"속성명\") 메소드를 활용하여 속성값을 추출할 수 있다.print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))print(soup.select_one(\"#Profile1 > div > a\").get(\"href\"))라. 반복되는 내용 추출라. 반복되는 내용 추출웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.웹페이지에 있는 세부 텍스트 내용은 통상 패러그래프 태그인 P 태그에 반복적으로 들어있기 때문에 아래와 같이 for문을 이용하여 추출할 수 있다.text =\"\"text =\"\"body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})body=soup.find(\"div\", {\"class\":\"post-body entry-content\"})for para in body.select(\"p\"):for para in body.select(\"p\"):text=text+para.text.strip()text=text+para.text.strip()texttext\n"
          ]
        }
      ]
    }
  ]
}